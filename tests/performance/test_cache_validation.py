#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Cache Performance Validation Test

Focus on validating the multi-tier cache system performance with >80% hit rate target.
"""

import json
import logging
import os
import sys
import tempfile
import time
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional

# Add MemMimic to path
test_dir = Path(__file__).parent
project_root = test_dir.parent.parent
sys.path.insert(0, str(project_root / 'src'))

try:
    from memmimic.memory.active.cache_manager import LRUMemoryCache, CachePool
    from memmimic.utils.caching import get_cache_statistics, clear_all_caches
except ImportError as e:
    print(f"Import error: {e}")
    print("Running basic cache validation without full MemMimic imports")

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


class CachePerformanceValidator:
    """Validator for cache performance testing"""
    
    def __init__(self):
        self.test_data = self._generate_test_data()
        
    def _generate_test_data(self) -> List[Dict[str, Any]]:
        """Generate test data for cache performance testing"""
        test_memories = []
        
        # Create diverse test memories for cache testing
        memory_templates = [
            "Cache performance optimization for {data_type} with {strategy}",
            "Multi-tier caching strategy improves {metric} by {improvement}",
            "LRU eviction policy manages memory usage efficiently for {use_case}",
            "TTL-based cache expiration ensures data freshness in {system}",
            "Cache hit rate monitoring detects performance degradation in {component}",
        ]
        
        data_types = ["embeddings", "search_results", "classifications", "metadata", "vectors"]
        strategies = ["memory_pooling", "lazy_loading", "prefetching", "compression", "sharding"]
        metrics = ["response_time", "memory_usage", "throughput", "latency", "bandwidth"]
        improvements = ["25%", "50%", "75%", "2x", "3x"]
        use_cases = ["search", "retrieval", "storage", "analysis", "processing"]
        systems = ["semantic_search", "memory_store", "api_layer", "processing_engine", "analytics"]
        components = ["search_engine", "cache_manager", "database_pool", "result_processor", "classifier"]
        
        # Generate diverse test data
        import itertools
        template_vars = [data_types, strategies, metrics, improvements, use_cases, systems, components]
        
        for i, template in enumerate(memory_templates):
            for j in range(40):  # 40 variations per template = 200 total items
                var_values = [vars[j % len(vars)] for vars in template_vars]
                content = template.format(
                    data_type=var_values[0],
                    strategy=var_values[1],
                    metric=var_values[2],
                    improvement=var_values[3],
                    use_case=var_values[4],
                    system=var_values[5],
                    component=var_values[6]
                )
                
                test_memories.append({
                    'id': i * 40 + j + 1,
                    'content': content,
                    'category': ['search', 'cache', 'performance'][j % 3],
                    'size': len(content)
                })
        
        logger.info(f"Generated {len(test_memories)} test memories for cache validation")
        return test_memories
        
    def validate_cache_performance(self) -> Dict[str, Any]:
        """Validate multi-tier cache system performance (>80% hit rate target)"""
        logger.info("ğŸ” Validating multi-tier cache performance...")
        
        cache_results = {}
        
        # Test different cache configurations
        cache_configs = {
            'small_cache': {'max_memory_mb': 64, 'max_items': 1000, 'default_ttl_seconds': 300},
            'medium_cache': {'max_memory_mb': 128, 'max_items': 2000, 'default_ttl_seconds': 600},
            'large_cache': {'max_memory_mb': 256, 'max_items': 5000, 'default_ttl_seconds': 1200}
        }
        
        try:
            for cache_name, config in cache_configs.items():
                cache = LRUMemoryCache(**config)
                cache_results[cache_name] = self._test_cache_instance(cache, cache_name)
                cache.shutdown()
                
            # Test cache pool performance
            pool_config = {
                'search_results': {'max_memory_mb': 128, 'max_items': 2000},
                'embeddings': {'max_memory_mb': 64, 'max_items': 1000},
                'classifications': {'max_memory_mb': 32, 'max_items': 500}
            }
            
            cache_pool = CachePool(pool_config)
            cache_results['pool_performance'] = self._test_cache_pool(cache_pool)
            cache_pool.shutdown_all()
            
        except Exception as e:
            logger.error(f"Cache testing failed: {e}")
            # Fallback to basic cache validation
            cache_results = self._basic_cache_validation()
            
        # Analyze overall cache performance
        overall_performance = self._analyze_cache_performance(cache_results)
        
        return {
            'cache_results': cache_results,
            'overall_performance': overall_performance,
            'validation_passed': overall_performance['average_hit_rate'] >= 0.8
        }
        
    def _basic_cache_validation(self) -> Dict[str, Any]:
        """Basic cache validation without full MemMimic integration"""
        logger.info("Running basic cache validation...")
        
        # Simple Python dict-based cache simulation
        basic_cache = {}
        hit_count = 0
        miss_count = 0
        
        # Warm up cache
        warm_up_data = self.test_data[:100]
        for item in warm_up_data:
            basic_cache[f"key_{item['id']}"] = item
            
        # Test cache performance
        test_operations = 1000
        
        for i in range(test_operations):
            if i % 3 == 0:  # Miss case - non-existent key
                key = f"nonexistent_key_{i}"
                if key in basic_cache:
                    hit_count += 1
                else:
                    miss_count += 1
            else:  # Hit case - existing key
                key_idx = i % len(warm_up_data)
                key = f"key_{warm_up_data[key_idx]['id']}"
                if key in basic_cache:
                    hit_count += 1
                else:
                    miss_count += 1
                    
        hit_rate = hit_count / (hit_count + miss_count) if (hit_count + miss_count) > 0 else 0
        
        return {
            'basic_cache': {
                'cache_name': 'basic_dict_cache',
                'hit_rate': hit_rate,
                'hit_count': hit_count,
                'miss_count': miss_count,
                'total_operations': test_operations,
                'performance_passed': hit_rate >= 0.8
            }
        }
        
    def _test_cache_instance(self, cache: 'LRUMemoryCache', cache_name: str) -> Dict[str, Any]:
        """Test individual cache instance performance"""
        
        # Warm up cache with test data
        warm_up_data = self.test_data[:100]
        for item in warm_up_data:
            cache.put(f"key_{item['id']}", item)
            
        # Performance test with mixed operations
        hit_count = 0
        miss_count = 0
        operation_times = []
        
        # Test pattern: 70% reads, 20% writes, 10% evictions
        test_operations = (['read'] * 70) + (['write'] * 20) + (['evict'] * 10)
        
        for i, operation in enumerate(test_operations):
            start_time = time.perf_counter()
            
            if operation == 'read':
                # Read existing keys (should hit) and some non-existent (should miss)
                if i % 3 == 0:  # Miss case
                    result = cache.get(f"nonexistent_key_{i}")
                    if result is None:
                        miss_count += 1
                    else:
                        hit_count += 1
                else:  # Hit case
                    key_idx = i % len(warm_up_data)
                    result = cache.get(f"key_{warm_up_data[key_idx]['id']}")
                    if result is not None:
                        hit_count += 1
                    else:
                        miss_count += 1
                        
            elif operation == 'write':
                # Write new data
                new_item = self.test_data[i % len(self.test_data)]
                cache.put(f"new_key_{i}", new_item)
                
            elif operation == 'evict':
                # Force some evictions by writing large items
                large_item = {'large_data': 'x' * 10000, 'id': f'large_{i}'}
                cache.put(f"large_key_{i}", large_item)
                
            operation_time = (time.perf_counter() - start_time) * 1000
            operation_times.append(operation_time)
            
        # Get final cache statistics
        cache_stats = cache.get_stats()
        
        hit_rate = hit_count / (hit_count + miss_count) if (hit_count + miss_count) > 0 else 0
        
        return {
            'cache_name': cache_name,
            'hit_rate': hit_rate,
            'hit_count': hit_count,
            'miss_count': miss_count,
            'total_operations': len(test_operations),
            'avg_operation_time_ms': sum(operation_times) / len(operation_times),
            'max_operation_time_ms': max(operation_times),
            'cache_stats': cache_stats,
            'performance_passed': hit_rate >= 0.8
        }
        
    def _test_cache_pool(self, cache_pool: 'CachePool') -> Dict[str, Any]:
        """Test cache pool performance"""
        
        # Test each cache in the pool
        pool_results = {}
        
        for cache_name in ['search_results', 'embeddings', 'classifications']:
            cache = cache_pool.get_cache(cache_name)
            if cache:
                pool_results[cache_name] = self._test_cache_instance(cache, cache_name)
                
        # Get pool-wide statistics
        pool_stats = cache_pool.get_pool_stats()
        
        return {
            'individual_caches': pool_results,
            'pool_stats': pool_stats
        }
        
    def _analyze_cache_performance(self, cache_results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze overall cache performance across all tests"""
        
        hit_rates = []
        operation_times = []
        
        # Collect metrics from individual cache tests
        for cache_name, results in cache_results.items():
            if cache_name == 'pool_performance':
                # Handle pool results
                for pool_cache_name, pool_results in results['individual_caches'].items():
                    hit_rates.append(pool_results['hit_rate'])
                    operation_times.append(pool_results['avg_operation_time_ms'])
            elif cache_name == 'basic_cache':
                # Handle basic cache results
                hit_rates.append(results['hit_rate'])
                operation_times.append(0.1)  # Estimated operation time for basic cache
            else:
                hit_rates.append(results['hit_rate'])
                operation_times.append(results['avg_operation_time_ms'])
                
        return {
            'average_hit_rate': sum(hit_rates) / len(hit_rates) if hit_rates else 0,
            'min_hit_rate': min(hit_rates) if hit_rates else 0,
            'max_hit_rate': max(hit_rates) if hit_rates else 0,
            'average_operation_time_ms': sum(operation_times) / len(operation_times) if operation_times else 0,
            'hit_rate_target_met': all(rate >= 0.8 for rate in hit_rates),
            'performance_grade': 'EXCELLENT' if all(rate >= 0.8 for rate in hit_rates) else 'NEEDS_IMPROVEMENT',
            'total_caches_tested': len(hit_rates)
        }
        
    def create_performance_report(self) -> Dict[str, Any]:
        """Create comprehensive cache performance report"""
        
        logger.info("ğŸ“Š Creating cache performance report...")
        
        # Run cache validation
        cache_validation = self.validate_cache_performance()
        
        # Generate recommendations
        recommendations = self._generate_recommendations(cache_validation)
        
        # Create performance report
        performance_report = {
            'validation_results': cache_validation,
            'recommendations': recommendations,
            'summary': {
                'validation_passed': cache_validation['validation_passed'],
                'average_hit_rate': cache_validation['overall_performance']['average_hit_rate'],
                'performance_grade': cache_validation['overall_performance']['performance_grade'],
                'target_achievement': cache_validation['overall_performance']['hit_rate_target_met']
            },
            'metadata': {
                'timestamp': time.time(),
                'date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'validator': 'Quality Agent Epsilon - Cache Validator',
                'test_data_size': len(self.test_data)
            }
        }
        
        # Save report to file
        report_file = project_root / 'cache_performance_report.json'
        with open(report_file, 'w') as f:
            json.dump(performance_report, f, indent=2)
            
        logger.info(f"ğŸ“‹ Cache performance report saved to {report_file}")
        
        return performance_report
        
    def _generate_recommendations(self, cache_validation: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on cache validation results"""
        
        recommendations = []
        
        overall_perf = cache_validation['overall_performance']
        hit_rate = overall_perf['average_hit_rate']
        
        if hit_rate >= 0.9:
            recommendations.append("ğŸ† Excellent cache performance! Consider this as a baseline for future optimizations.")
            recommendations.append("ğŸ“Š Monitor cache performance continuously to maintain high hit rates.")
        elif hit_rate >= 0.8:
            recommendations.append("âœ… Cache performance meets target. Consider fine-tuning for even better results.")
            recommendations.append("ğŸ” Analyze cache miss patterns to identify optimization opportunities.")
        elif hit_rate >= 0.6:
            recommendations.append("âš ï¸ Cache performance below target. Consider increasing cache sizes or adjusting TTL values.")
            recommendations.append("ğŸ”§ Review cache eviction policies and memory limits.")
        else:
            recommendations.append("ğŸš¨ Poor cache performance requires immediate attention.")
            recommendations.append("ğŸ“ˆ Consider redesigning cache architecture or increasing resources.")
            
        # Specific recommendations based on results
        if not overall_perf['hit_rate_target_met']:
            recommendations.append("ğŸ¯ Implement cache warming strategies for frequently accessed data.")
            recommendations.append("âš¡ Consider implementing cache prefetching for predictable access patterns.")
            
        recommendations.append("ğŸ“š Implement automated cache performance monitoring and alerting.")
        recommendations.append("ğŸ”„ Establish regular cache performance regression testing.")
        
        return recommendations


def run_cache_validation():
    """Main function to run cache performance validation"""
    
    print("ğŸš€ Starting Cache Performance Validation...")
    print("="*60)
    
    validator = CachePerformanceValidator()
    
    try:
        # Create performance report
        report = validator.create_performance_report()
        
        # Display results
        print(f"\nğŸ¯ CACHE PERFORMANCE VALIDATION RESULTS")
        print("="*60)
        
        summary = report['summary']
        print(f"âœ… Validation Status: {'PASSED' if summary['validation_passed'] else 'FAILED'}")
        print(f"ğŸ“Š Average Hit Rate: {summary['average_hit_rate']:.1%}")
        print(f"ğŸ† Performance Grade: {summary['performance_grade']}")
        print(f"ğŸ¯ Target Achievement: {'YES' if summary['target_achievement'] else 'NO'}")
        
        print(f"\nğŸ’¡ KEY RECOMMENDATIONS:")
        for i, recommendation in enumerate(report['recommendations'][:5], 1):
            print(f"  {i}. {recommendation}")
            
        print(f"\nğŸ“„ Full report saved to: cache_performance_report.json")
        print("="*60)
        
        return report
        
    except Exception as e:
        logger.error(f"Cache validation failed: {e}")
        print(f"âŒ Validation failed: {e}")
        return None


if __name__ == "__main__":
    run_cache_validation()